{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP40IesF9HeralurJDgS0GQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "# 1. Initialize the Environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# 2. Q-learning parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "\n",
        "num_episodes = 5000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "# 3. Initialize Q-table\n",
        "# Q-table dimensions: (number of states, number of actions)\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "\n",
        "def implement_bellman_equation(\n",
        "    q_table,\n",
        "    state,\n",
        "    action,\n",
        "    alpha,\n",
        "    reward,\n",
        "    gamma,\n",
        "    new_state\n",
        "):\n",
        "      q_table[state, action] = (\n",
        "          q_table[state, action] * (1 - alpha) +\n",
        "          alpha * (reward + gamma * np.max(q_table[new_state, :]))\n",
        "      )\n",
        "      return q_table"
      ],
      "metadata": {
        "id": "3ltT_6X5UTKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vdir(obj):\n",
        "    return [x for x in dir(obj) if not x.startswith('_')]\n",
        "\n",
        "\n",
        "vdir(env)\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2qzzPTV9yl",
        "outputId": "ce687730-ac41-4750-d061-cff1a0ed500c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/toy_text/taxi.py:443: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf-fuzilnM-2",
        "outputId": "16d10a98-e544-4829-d2fb-7707ed6adf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "Episode: 1000/5000 - Average reward: -104.811\n",
            "Episode: 2000/5000 - Average reward: -11.654\n",
            "Episode: 3000/5000 - Average reward: 1.539\n",
            "Episode: 4000/5000 - Average reward: 2.406\n",
            "Episode: 5000/5000 - Average reward: 2.303\n",
            "Training finished!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. Training loop\n",
        "rewards_per_episode = []\n",
        "\n",
        "print(\"Training started...\")\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0] # reset() returns (observation, info)\n",
        "    done = False\n",
        "    rewards_current_episode = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Exploration-exploitation trade-off\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state,:]) # Exploit learned values\n",
        "\n",
        "        # Take action and observe new state and reward\n",
        "        new_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        # Update Q-table using the Bellman equation\n",
        "        q_table = implement_bellman_equation(\n",
        "          q_table,\n",
        "          state,\n",
        "          action,\n",
        "          alpha,\n",
        "          reward,\n",
        "          gamma,\n",
        "          new_state\n",
        "        )\n",
        "\n",
        "        state = new_state\n",
        "        rewards_current_episode += reward\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    rewards_per_episode.append(rewards_current_episode)\n",
        "\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Episode: {episode + 1}/{num_episodes} - Average reward: {np.mean(rewards_per_episode[-1000:])}\")\n",
        "\n",
        "print(\"Training finished!\\n\")\n",
        "\n"
      ]
    }
  ]
}